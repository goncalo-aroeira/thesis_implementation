{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "dfee0f7f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saving all artifacts under: /home/guests3/gba/thesis_implementation/thesis_implementation/single_cell/pan_cancer/data/processed/xfer_shared\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# 📦 Shared output directory for all artifacts (Pancancer + Tahoe)\n",
    "\n",
    "from pathlib import Path\n",
    "BASE_DIR = Path(\"../../data/processed/xfer_shared\")  # <- change if you want\n",
    "BASE_DIR.mkdir(parents=True, exist_ok=True)\n",
    "print(\"Saving all artifacts under:\", BASE_DIR.resolve())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "addba549",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "📥 Loading expression + AnnData...\n"
     ]
    }
   ],
   "source": [
    "# %% [markdown]\n",
    "# 🧬 Pancancer: PCA on single-cell expression → per-cell PCs → pseudo-bulk PCs (mean per group)\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "from sklearn.decomposition import PCA, IncrementalPCA\n",
    "import joblib\n",
    "import scanpy as sc\n",
    "\n",
    "# ===== CONFIG =====\n",
    "EXP_PATH    = \"../../data/processed/pancancer_sc_expression.parquet\"  # cells × genes (wide)\n",
    "META_PATH   = \"../../data/processed/pancancer_dimred.h5ad\"\n",
    "GROUP_KEY   = \"SIDM\"            # how to aggregate to pseudo-bulk (SANGER-like group)\n",
    "N_PCS       = 30                # fixed at 30 PCs\n",
    "USE_INCREMENTAL = False         # flip to True if memory is tight\n",
    "BATCH_SIZE  = 20000             # only used if USE_INCREMENTAL=True\n",
    "\n",
    "# ----- Load & align -----\n",
    "print(\"📥 Loading expression + AnnData...\")\n",
    "X = pd.read_parquet(EXP_PATH)            # cells × genes\n",
    "adata = sc.read(META_PATH)\n",
    "cell_ids = adata.obs_names.to_numpy()\n",
    "groups = adata.obs[GROUP_KEY].to_numpy()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f050b939",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "🧬 EXP raw shape: (39715, 30314) (rows × cols)\n",
      "🧬 AnnData cells: 39,715\n",
      "\n",
      "🧾 EXP index:\n",
      "  name: SIDM | dtype: category\n",
      "  first 5 index values: ['SIDM00890', 'SIDM00965', 'SIDM00333', 'SIDM01101', 'SIDM00722']\n",
      "\n",
      "🧾 EXP columns (first 10):\n",
      "['A1BG', 'A1BG-AS1', 'A1CF', 'A2M', 'A2M-AS1', 'A2ML1', 'A2ML1-AS1', 'A2ML1-AS2', 'A3GALT2', 'A4GALT']\n",
      "  # numeric columns: 30314\n",
      "  # object columns : 0\n",
      "\n",
      "ℹ️ No obvious ID column names detected in EXP (we'll probe string columns).\n",
      "\n",
      "📇 AnnData obs_names (first 5): ['C32_SKIN', 'NCIH446_LUNG', 'MFE319_ENDOMETRIUM', 'SKNAS_AUTONOMIC_GANGLIA', 'NCIH2452_PLEURA']\n",
      "🧪 GROUP_KEY value counts (top 5):\n",
      "SIDM\n",
      "SIDM00703    1994\n",
      "SIDM00144    1185\n",
      "SIDM01082    1079\n",
      "SIDM01060     833\n",
      "SIDM00806     832\n",
      "Name: count, dtype: int64\n",
      "\n",
      "🔗 Overlap: EXP.index ∩ obs_names = 0\n",
      "🔗 Overlap: EXP.columns ∩ obs_names = 0\n",
      "\n",
      "⚠️ No string column in EXP overlaps with obs_names.\n",
      "\n",
      "⚠️ EXP index has duplicates. Examples: ['SIDM00890', 'SIDM00965', 'SIDM00333', 'SIDM01101', 'SIDM00722']\n",
      "✅ AnnData obs_names is unique\n",
      "\n",
      "🧪 Sample of EXP index (5): ['SIDM00890', 'SIDM00965', 'SIDM00333', 'SIDM01101', 'SIDM00722']\n",
      "🧪 Sample of EXP columns (5): ['A1BG', 'A1BG-AS1', 'A1CF', 'A2M', 'A2M-AS1']\n",
      "\n",
      "🟩 Done printing. Send me the cell output above and I'll adapt the PCA code accordingly.\n"
     ]
    }
   ],
   "source": [
    "from collections import Counter\n",
    "\n",
    "obs_names = adata.obs_names.astype(str)\n",
    "\n",
    "\n",
    "print(f\"🧬 EXP raw shape: {X.shape} (rows × cols)\")\n",
    "print(f\"🧬 AnnData cells: {adata.n_obs:,}\")\n",
    "\n",
    "# ---- Peek at EXP table ----\n",
    "print(\"\\n🧾 EXP index:\")\n",
    "print(\"  name:\", X.index.name, \"| dtype:\", getattr(X.index, \"dtype\", None))\n",
    "print(\"  first 5 index values:\", list(map(str, X.index[:5])) if len(X.index) else [])\n",
    "\n",
    "print(\"\\n🧾 EXP columns (first 10):\")\n",
    "print(list(X.columns[:10]))\n",
    "print(\"  # numeric columns:\", X.select_dtypes(include=[np.number]).shape[1])\n",
    "print(\"  # object columns :\", X.select_dtypes(exclude=[np.number]).shape[1])\n",
    "\n",
    "# If a likely cell-id column exists, show a preview\n",
    "likely_id_cols = [c for c in X.columns if c.lower() in {\n",
    "    \"cell_id\",\"cellid\",\"barcode\",\"cell\",\"obs_names\",\"obsname\",\"cell_barcode\",\"cellbarcode\"\n",
    "}]\n",
    "if likely_id_cols:\n",
    "    print(\"\\n🔎 Found candidate ID columns in EXP:\", likely_id_cols)\n",
    "    for c in likely_id_cols:\n",
    "        print(f\"  {c}: first 5 ->\", X[c].astype(str).head().tolist())\n",
    "else:\n",
    "    print(\"\\nℹ️ No obvious ID column names detected in EXP (we'll probe string columns).\")\n",
    "\n",
    "# ---- AnnData previews ----\n",
    "print(\"\\n📇 AnnData obs_names (first 5):\", obs_names[:5].tolist())\n",
    "print(\"🧪 GROUP_KEY value counts (top 5):\")\n",
    "print(adata.obs[GROUP_KEY].value_counts().head())\n",
    "\n",
    "# ---- Overlap diagnostics ----\n",
    "# 1) overlap between EXP index and obs_names\n",
    "index_overlap = len(set(map(str, X.index)) & set(obs_names))\n",
    "print(f\"\\n🔗 Overlap: EXP.index ∩ obs_names = {index_overlap}\")\n",
    "\n",
    "# 2) overlap between EXP columns and obs_names (signals transposed matrix)\n",
    "cols_overlap = len(set(map(str, X.columns)) & set(obs_names))\n",
    "print(f\"🔗 Overlap: EXP.columns ∩ obs_names = {cols_overlap}\")\n",
    "\n",
    "# 3) try all short string-like columns as candidate ID columns\n",
    "str_cols = X.select_dtypes(exclude=[np.number]).columns.tolist()\n",
    "cand_scores = []\n",
    "for c in str_cols:\n",
    "    vals = X[c].astype(str)\n",
    "    ov = len(set(vals) & set(obs_names))\n",
    "    if ov > 0:\n",
    "        cand_scores.append((c, ov))\n",
    "cand_scores.sort(key=lambda t: t[1], reverse=True)\n",
    "\n",
    "if cand_scores:\n",
    "    print(\"\\n🏷️ Candidate ID columns ranked by overlap with obs_names:\")\n",
    "    for c, ov in cand_scores[:10]:\n",
    "        print(f\"  {c:30s} -> overlap {ov}\")\n",
    "else:\n",
    "    print(\"\\n⚠️ No string column in EXP overlaps with obs_names.\")\n",
    "\n",
    "# ---- Duplicate checks ----\n",
    "if X.index.is_unique:\n",
    "    print(\"\\n✅ EXP index is unique\")\n",
    "else:\n",
    "    dup_idx = [k for k, v in Counter(map(str, X.index)).items() if v > 1][:5]\n",
    "    print(\"\\n⚠️ EXP index has duplicates. Examples:\", dup_idx)\n",
    "\n",
    "if adata.obs_names.is_unique:\n",
    "    print(\"✅ AnnData obs_names is unique\")\n",
    "else:\n",
    "    dup_obs = [k for k, v in Counter(map(str, adata.obs_names)).items() if v > 1][:5]\n",
    "    print(\"⚠️ AnnData obs_names has duplicates. Examples:\", dup_obs)\n",
    "\n",
    "# ---- Small samples to inspect visually ----\n",
    "print(\"\\n🧪 Sample of EXP index (5):\", list(map(str, X.index[:5])) if len(X.index) else [])\n",
    "print(\"🧪 Sample of EXP columns (5):\", list(map(str, X.columns[:5])))\n",
    "\n",
    "# If there is some overlap, show a few matching IDs (index or columns)\n",
    "if index_overlap > 0:\n",
    "    matches = list(set(map(str, X.index)) & set(obs_names))\n",
    "    print(\"✅ Example matching IDs (index):\", matches[:5])\n",
    "if cols_overlap > 0:\n",
    "    matches_c = list(set(map(str, X.columns)) & set(obs_names))\n",
    "    print(\"✅ Example matching IDs (columns):\", matches_c[:5])\n",
    "\n",
    "print(\"\\n🟩 Done printing. Send me the cell output above and I'll adapt the PCA code accordingly.\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "e0264f54",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "✅ Matrix for PCA: 39,715 cells × 30,314 genes\n",
      "✅ Group vector length: 39,715 (e.g., ['SIDM00078' 'SIDM00080' 'SIDM00082' 'SIDM00088' 'SIDM00092'])\n",
      "💾 Saved: pancancer_cell_PCs_30.parquet pancancer_pseudobulk_30PCs.parquet pancancer_expr_pca.joblib pancancer_gene_columns.txt\n"
     ]
    }
   ],
   "source": [
    "# X index holds the SIDM group IDs (duplicated), not per-cell IDs.\n",
    "# We'll use that as the grouping label and skip obs_names alignment.\n",
    "# Keep only numeric gene columns and cast to float32 for memory.\n",
    "\n",
    "# Keep only numeric genes\n",
    "X = X.select_dtypes(include=[np.number]).astype(np.float32)\n",
    "\n",
    "# Use SIDM from the index as group labels (safer given your printout)\n",
    "if str(getattr(X.index, \"name\", \"\")).upper() == \"SIDM\":\n",
    "    groups = X.index.astype(str).to_numpy()\n",
    "else:\n",
    "    # Fallback: rely on AnnData if index isn't SIDM\n",
    "    assert len(X) == adata.n_obs, \"Row count mismatch; cannot attach group labels.\"\n",
    "    groups = adata.obs[GROUP_KEY].astype(str).to_numpy()\n",
    "\n",
    "print(f\"✅ Matrix for PCA: {X.shape[0]:,} cells × {X.shape[1]:,} genes\")\n",
    "print(f\"✅ Group vector length: {len(groups):,} (e.g., {np.unique(groups)[:5]})\")\n",
    "\n",
    "# ----- Fit PCA (fixed 30 PCs) -----\n",
    "if USE_INCREMENTAL:\n",
    "    pca = IncrementalPCA(n_components=N_PCS, batch_size=BATCH_SIZE)\n",
    "    for start in range(0, X.shape[0], BATCH_SIZE):\n",
    "        pca.partial_fit(X.iloc[start:start+BATCH_SIZE].values)\n",
    "    PCs_list = []\n",
    "    for start in range(0, X.shape[0], BATCH_SIZE):\n",
    "        PCs_list.append(pca.transform(X.iloc[start:start+BATCH_SIZE].values))\n",
    "    PCs = np.vstack(PCs_list)\n",
    "else:\n",
    "    # randomized SVD is faster and lighter for tall/wide matrices\n",
    "    pca = PCA(n_components=N_PCS, svd_solver=\"randomized\", random_state=13)  # no whitening\n",
    "    PCs = pca.fit_transform(X.values)\n",
    "\n",
    "pc_cols = [f\"PC{i}\" for i in range(1, N_PCS + 1)]\n",
    "pc_df = pd.DataFrame(PCs, index=X.index, columns=pc_cols)\n",
    "pc_df[\"group\"] = groups\n",
    "\n",
    "# ----- Pseudo-bulk aggregation (mean per group) -----\n",
    "pseudo_bulk = (pc_df\n",
    "               .groupby(\"group\")[pc_cols]\n",
    "               .mean()\n",
    "               .rename_axis(\"SANGER_MODEL_ID\")\n",
    "               .reset_index())\n",
    "\n",
    "# ----- Save everything important to BASE_DIR -----\n",
    "(pc_df.drop(columns=[\"group\"])\n",
    "      .to_parquet(BASE_DIR / \"pancancer_cell_PCs_30.parquet\"))\n",
    "pseudo_bulk.to_parquet(BASE_DIR / \"pancancer_pseudobulk_30PCs.parquet\")\n",
    "joblib.dump(pca, BASE_DIR / \"pancancer_expr_pca.joblib\")\n",
    "pd.Series(X.columns, name=\"gene\").to_csv(BASE_DIR / \"pancancer_gene_columns.txt\",\n",
    "                                         index=False, header=False)\n",
    "\n",
    "print(\"💾 Saved:\",\n",
    "      (BASE_DIR / \"pancancer_cell_PCs_30.parquet\").name,\n",
    "      (BASE_DIR / \"pancancer_pseudobulk_30PCs.parquet\").name,\n",
    "      (BASE_DIR / \"pancancer_expr_pca.joblib\").name,\n",
    "      (BASE_DIR / \"pancancer_gene_columns.txt\").name)\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tese-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
