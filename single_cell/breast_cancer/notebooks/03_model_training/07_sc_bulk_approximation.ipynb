{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "a383b5a0",
   "metadata": {},
   "outputs": [],
   "source": [
    "# Paths already used in your notebook\n",
    "SC_PARQUET   = \"../../data/filtered_datasets/sc_overlap_genes.parquet\"   # cells × genes, index=SANGER_MODEL_ID\n",
    "BULK_LONG    = \"../../data/gdsc_bulk_overlap_genes.parquet\"              # long bulk with LN_IC50 + SIDG genes as columns\n",
    "MODEL_DIR    = \"models\"\n",
    "\n",
    "# Reuse helpers from before, or keep these minimal ones:\n",
    "import numpy as np, pandas as pd, os, re, joblib\n",
    "pd.set_option(\"display.width\", 160)\n",
    "pd.set_option(\"display.max_columns\", 30)\n",
    "\n",
    "def load_sc_df(path):\n",
    "    df = pd.read_parquet(path)\n",
    "    if df.index.name != \"SANGER_MODEL_ID\":\n",
    "        df.index.name = \"SANGER_MODEL_ID\"\n",
    "    # add synthetic cell_id\n",
    "    if \"cell_id\" not in df.columns:\n",
    "        seq = df.groupby(level=0).cumcount().astype(str)\n",
    "        df.insert(0, \"cell_id\", df.index.astype(str) + \"__\" + seq)\n",
    "    return df\n",
    "\n",
    "def load_bulk_wide_with_genes(path):\n",
    "    df = pd.read_parquet(path) if path.endswith(\".parquet\") else pd.read_csv(path)\n",
    "    # pull out LN_IC50 long view (unused here) and a features matrix (samples × genes)\n",
    "    # Assume columns: SANGER_MODEL_ID, DRUG_ID, LN_IC50, SIDG...\n",
    "    # Keep the expression columns (SIDG*) by pattern:\n",
    "    feature_cols = [c for c in df.columns if c.startswith(\"SIDG\")]\n",
    "    expr = df[[\"SANGER_MODEL_ID\"] + feature_cols].drop_duplicates(\"SANGER_MODEL_ID\").set_index(\"SANGER_MODEL_ID\")\n",
    "    expr.index = expr.index.astype(str)\n",
    "    return expr  # voom-like bulk features used to train\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "ea5d5008",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "pseudo-bulk shape: (28, 2044)\n"
     ]
    }
   ],
   "source": [
    "# Build pseudo-bulk by averaging cells per line (use mean of log1p CP10k; median also works)\n",
    "sc = load_sc_df(SC_PARQUET)\n",
    "gene_cols = [c for c in sc.columns if c.startswith(\"SIDG\")]\n",
    "sc_g = sc[gene_cols].astype(float)\n",
    "\n",
    "sc_pseudobulk = sc_g.groupby(sc.index).mean()  # index=SANGER_MODEL_ID, cols=genes\n",
    "print(\"pseudo-bulk shape:\", sc_pseudobulk.shape)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "e99dbf64",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "calibration per-gene (head):\n"
     ]
    },
    {
     "data": {
      "application/vnd.microsoft.datawrangler.viewer.v0+json": {
       "columns": [
        {
         "name": "index",
         "rawType": "object",
         "type": "string"
        },
        {
         "name": "slope",
         "rawType": "float64",
         "type": "float"
        },
        {
         "name": "intercept",
         "rawType": "float64",
         "type": "float"
        }
       ],
       "ref": "528830c2-32b6-491c-95ca-56c057f2c19d",
       "rows": [
        [
         "SIDG00004",
         "267.50308237396774",
         "-2.873278287650516"
        ],
        [
         "SIDG00036",
         "2.591733912160817",
         "7.461720559661194"
        ],
        [
         "SIDG00100",
         "1.5019391919294465",
         "6.148303196683216"
        ],
        [
         "SIDG00101",
         "0.7857752295484156",
         "6.585975231207497"
        ],
        [
         "SIDG00147",
         "4.922744264709482",
         "4.582055974950184"
        ]
       ],
       "shape": {
        "columns": 2,
        "rows": 5
       }
      },
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>slope</th>\n",
       "      <th>intercept</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>SIDG00004</th>\n",
       "      <td>267.503082</td>\n",
       "      <td>-2.873278</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIDG00036</th>\n",
       "      <td>2.591734</td>\n",
       "      <td>7.461721</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIDG00100</th>\n",
       "      <td>1.501939</td>\n",
       "      <td>6.148303</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIDG00101</th>\n",
       "      <td>0.785775</td>\n",
       "      <td>6.585975</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>SIDG00147</th>\n",
       "      <td>4.922744</td>\n",
       "      <td>4.582056</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "                slope  intercept\n",
       "SIDG00004  267.503082  -2.873278\n",
       "SIDG00036    2.591734   7.461721\n",
       "SIDG00100    1.501939   6.148303\n",
       "SIDG00101    0.785775   6.585975\n",
       "SIDG00147    4.922744   4.582056"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    }
   ],
   "source": [
    "bulk_voom = load_bulk_wide_with_genes(BULK_LONG)\n",
    "# intersect lines and genes\n",
    "common_lines = sc_pseudobulk.index.intersection(bulk_voom.index)\n",
    "common_genes = sc_pseudobulk.columns.intersection(bulk_voom.columns)\n",
    "Xpb = sc_pseudobulk.loc[common_lines, common_genes]\n",
    "Ybk = bulk_voom.loc[common_lines, common_genes]\n",
    "\n",
    "# closed-form per-gene regression (OLS): a = cov(X,Y)/var(X), b = mean(Y) - a*mean(X)\n",
    "mu_x = Xpb.mean(axis=0)\n",
    "mu_y = Ybk.mean(axis=0)\n",
    "var_x = Xpb.var(axis=0, ddof=1)\n",
    "cov_xy = ((Xpb - mu_x) * (Ybk - mu_y)).mean(axis=0) * (len(common_lines)/(len(common_lines)-1))  # unbiased\n",
    "\n",
    "a = (cov_xy / var_x).replace([np.inf, -np.inf], np.nan).fillna(0.0)\n",
    "b = (mu_y - a * mu_x).fillna(mu_y)  # if a==0 fallback to mean alignment\n",
    "\n",
    "calib_gene = pd.DataFrame({\"slope\": a, \"intercept\": b})\n",
    "print(\"calibration per-gene (head):\")\n",
    "display(calib_gene.head())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "5054bb6c",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "bulkified preds shape: (308270, 4)\n"
     ]
    }
   ],
   "source": [
    "# Cell B3 (fixed): apply gene-wise map to every cell and predict (helpers included)\n",
    "\n",
    "import os, re, joblib\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# helpers (self-contained)\n",
    "def available_models(model_dir, allowed_drugs=None):\n",
    "    paths = {}\n",
    "    if not (model_dir and os.path.isdir(model_dir)):\n",
    "        return paths\n",
    "    allow = None if allowed_drugs is None else set(map(str, allowed_drugs))\n",
    "    for fname in os.listdir(model_dir):\n",
    "        if not fname.endswith(\".joblib\"):\n",
    "            continue\n",
    "        m = re.search(r\"(\\d+)\\.joblib$\", fname)  # capture trailing digits\n",
    "        if not m:\n",
    "            continue\n",
    "        d = m.group(1)\n",
    "        if allow is None or d in allow:\n",
    "            paths[d] = os.path.join(model_dir, fname)\n",
    "    return paths\n",
    "\n",
    "def load_bundle(path):\n",
    "    b = joblib.load(path)\n",
    "    for k in (\"model\", \"scaler\", \"gene_cols\"):\n",
    "        if k not in b:\n",
    "            raise KeyError(f\"Bundle {os.path.basename(path)} missing key: {k}\")\n",
    "    b[\"gene_cols\"] = [str(g) for g in b[\"gene_cols\"]]\n",
    "    return b\n",
    "\n",
    "# --- apply the per-gene affine mapping learned in B2 to ALL cells ---\n",
    "# expects: SC_PARQUET, calib_gene (with 'slope','intercept'), common_genes from B2\n",
    "\n",
    "# reload SC to be safe (uses same synthetic cell_id logic as before)\n",
    "sc = pd.read_parquet(SC_PARQUET)\n",
    "if sc.index.name != \"SANGER_MODEL_ID\":\n",
    "    sc.index.name = \"SANGER_MODEL_ID\"\n",
    "if \"cell_id\" not in sc.columns:\n",
    "    seq = sc.groupby(level=0).cumcount().astype(str)\n",
    "    sc.insert(0, \"cell_id\", sc.index.astype(str) + \"__\" + seq)\n",
    "\n",
    "# keep only genes we have calibration for\n",
    "genes_use = list(set(common_genes).intersection(calib_gene.index))\n",
    "sc_g = sc[genes_use].astype(float)\n",
    "\n",
    "A = calib_gene.loc[genes_use, \"slope\"]\n",
    "B = calib_gene.loc[genes_use, \"intercept\"]\n",
    "\n",
    "# x' = a_g * x + b_g\n",
    "sc_bulkified = sc_g.mul(A, axis=1).add(B, axis=1)\n",
    "\n",
    "# --- predict with existing bundles on bulkified features ---\n",
    "model_paths = available_models(MODEL_DIR, allowed_drugs=None)\n",
    "if not model_paths:\n",
    "    raise ValueError(f\"No model bundles found in {MODEL_DIR}\")\n",
    "\n",
    "pred_rows = []\n",
    "skipped = []\n",
    "\n",
    "for d, p in model_paths.items():\n",
    "    b = load_bundle(p)\n",
    "    want = [g for g in b[\"gene_cols\"] if g in sc_bulkified.columns]\n",
    "    miss = [g for g in b[\"gene_cols\"] if g not in sc_bulkified.columns]\n",
    "    if miss:\n",
    "        skipped.append((d, len(miss)))\n",
    "        continue\n",
    "    X = sc_bulkified[b[\"gene_cols\"]].values\n",
    "    Xs = b[\"scaler\"].transform(X)\n",
    "    y = b[\"model\"].predict(Xs)\n",
    "    pred_rows.append(pd.DataFrame({\n",
    "        \"cell_id\": sc[\"cell_id\"].values,\n",
    "        \"SANGER_MODEL_ID\": sc.index.astype(str).values,\n",
    "        \"drug_id\": d,\n",
    "        \"y_pred\": y\n",
    "    }))\n",
    "\n",
    "preds_bulkified = (pd.concat(pred_rows, ignore_index=True)\n",
    "                   if pred_rows else pd.DataFrame(columns=[\"cell_id\",\"SANGER_MODEL_ID\",\"drug_id\",\"y_pred\"]))\n",
    "\n",
    "print(f\"bulkified preds shape: {preds_bulkified.shape}\")\n",
    "if skipped:\n",
    "    print(\"[info] skipped bundles due to missing genes:\", skipped[:10], \n",
    "          (\"... (truncated)\" if len(skipped) > 10 else \"\"))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "id": "514d43d3",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "=== Agreement rate (bulk within per-cell [25,75]) ===\n",
      "        original preds: nan\n",
      "        moment matched: nan\n",
      "      bulkified (gene): 0.12043795620437957\n"
     ]
    }
   ],
   "source": [
    "# Safe agreement comparison for whichever prediction tables exist\n",
    "\n",
    "import numpy as np\n",
    "import pandas as pd\n",
    "\n",
    "# Build bulk LN_IC50 wide from your long table (robust to parquet/csv)\n",
    "def build_bulk_LNwide(path):\n",
    "    df = pd.read_parquet(path) if path.endswith(\".parquet\") else pd.read_csv(path)\n",
    "    # expect columns like: SANGER_MODEL_ID, DRUG_ID, LN_IC50\n",
    "    # fall back to lower/variant names if needed\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    line_col = cols.get(\"sanger_model_id\") or cols.get(\"cell_line\") or \"SANGER_MODEL_ID\"\n",
    "    drug_col = cols.get(\"drug_id\") or cols.get(\"drug\") or \"DRUG_ID\"\n",
    "    ln_col   = cols.get(\"ln_ic50\") or cols.get(\"lnic50\") or \"LN_IC50\"\n",
    "    if not {line_col, drug_col, ln_col} <= set(df.columns):\n",
    "        raise KeyError(f\"Bulk long is missing required columns. Found: {list(df.columns)}\")\n",
    "    w = df.pivot_table(index=line_col, columns=drug_col, values=ln_col, aggfunc=\"mean\")\n",
    "    w.index = w.index.astype(str); w.columns = w.columns.astype(str)\n",
    "    return w\n",
    "\n",
    "def percentile_of_value(arr, value):\n",
    "    arr = np.asarray(arr, float)\n",
    "    if arr.size == 0 or np.isnan(value): return np.nan\n",
    "    lt = np.sum(arr < value); eq = np.sum(arr == value)\n",
    "    return 100.0 * (lt + 0.5*eq) / arr.size\n",
    "\n",
    "def agreement_rate(preds_df, lnwide, agree_band=(25,75)):\n",
    "    if preds_df is None or len(preds_df)==0: return np.nan\n",
    "    ok = 0; total = 0\n",
    "    for (line, drug), sub in preds_df.groupby([\"SANGER_MODEL_ID\",\"drug_id\"]):\n",
    "        if (line in lnwide.index) and (drug in lnwide.columns):\n",
    "            pct = percentile_of_value(sub[\"y_pred\"].values, float(lnwide.loc[line, drug]))\n",
    "            if not np.isnan(pct):\n",
    "                total += 1\n",
    "                ok += (agree_band[0] <= pct <= agree_band[1])\n",
    "    return (ok / total) if total else np.nan\n",
    "\n",
    "LNwide = build_bulk_LNwide(BULK_LONG)\n",
    "\n",
    "# Probe which prediction DataFrames exist in the namespace\n",
    "candidates = []\n",
    "candidates.append((\"original preds\",    globals().get(\"preds\", None)))\n",
    "candidates.append((\"moment matched\",    globals().get(\"preds_mm\", None)))\n",
    "candidates.append((\"bulkified (gene)\",  globals().get(\"preds_bulkified\", None)))\n",
    "\n",
    "print(\"=== Agreement rate (bulk within per-cell [25,75]) ===\")\n",
    "for name, df in candidates:\n",
    "    try:\n",
    "        rate = agreement_rate(df, LNwide)\n",
    "    except Exception as e:\n",
    "        rate = f\"error: {e}\"\n",
    "    print(f\"{name:>22}: {rate}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "id": "1ff35476",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Baseline agreement rates (bulk within [25,75]%):\n",
      "   bulkified_gene: 0.120\n",
      "\n",
      "Chosen baseline: bulkified_gene\n"
     ]
    }
   ],
   "source": [
    "# Cell 1: choose the best baseline predictions automatically\n",
    "\n",
    "import numpy as np, pandas as pd, os, matplotlib.pyplot as plt\n",
    "\n",
    "# helper to build bulk LN_IC50 wide\n",
    "def build_bulk_LNwide(path):\n",
    "    df = pd.read_parquet(path) if path.endswith(\".parquet\") else pd.read_csv(path)\n",
    "    cols = {c.lower(): c for c in df.columns}\n",
    "    line_col = cols.get(\"sanger_model_id\") or cols.get(\"cell_line\") or \"SANGER_MODEL_ID\"\n",
    "    drug_col = cols.get(\"drug_id\") or cols.get(\"drug\") or \"DRUG_ID\"\n",
    "    ln_col   = cols.get(\"ln_ic50\") or cols.get(\"lnic50\") or \"LN_IC50\"\n",
    "    w = df.pivot_table(index=line_col, columns=drug_col, values=ln_col, aggfunc=\"mean\")\n",
    "    w.index = w.index.astype(str); w.columns = w.columns.astype(str)\n",
    "    return w\n",
    "\n",
    "def percentile_of_value(arr, value):\n",
    "    arr = np.asarray(arr, float)\n",
    "    if arr.size == 0 or np.isnan(value): return np.nan\n",
    "    lt = np.sum(arr < value); eq = np.sum(arr == value)\n",
    "    return 100.0 * (lt + 0.5*eq) / arr.size\n",
    "\n",
    "def agreement_rate(preds_df, lnwide, band=(25,75)):\n",
    "    if preds_df is None or len(preds_df)==0: return np.nan\n",
    "    ok = 0; total = 0\n",
    "    for (line, drug), sub in preds_df.groupby([\"SANGER_MODEL_ID\",\"drug_id\"]):\n",
    "        if (line in lnwide.index) and (drug in lnwide.columns):\n",
    "            pct = percentile_of_value(sub[\"y_pred\"].values, float(lnwide.loc[line, drug]))\n",
    "            if not np.isnan(pct):\n",
    "                total += 1\n",
    "                ok += (band[0] <= pct <= band[1])\n",
    "    return (ok / total) if total else np.nan\n",
    "\n",
    "LNwide = build_bulk_LNwide(BULK_LONG)\n",
    "\n",
    "# candidates from your session (some may not exist)\n",
    "candidates = {\n",
    "    \"original\":       globals().get(\"preds\", None),\n",
    "    \"moment_matched\": globals().get(\"preds_mm\", None),\n",
    "    \"bulkified_gene\": globals().get(\"preds_bulkified\", None),\n",
    "}\n",
    "\n",
    "rates = {name: agreement_rate(df, LNwide) for name, df in candidates.items() if df is not None and len(df)}\n",
    "best_name, best_df = max(rates.items(), key=lambda kv: (kv[1] if kv[1]==kv[1] else -1)) if rates else (None, None)\n",
    "\n",
    "print(\"Baseline agreement rates (bulk within [25,75]%):\")\n",
    "for k,v in rates.items(): print(f\"  {k:>15}: {v:.3f}\")\n",
    "print(f\"\\nChosen baseline: {best_name}\")\n",
    "BASE_PREDS = candidates.get(best_name, None)\n",
    "assert BASE_PREDS is not None and len(BASE_PREDS), \"No predictions available to proceed.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 11,
   "id": "673be915",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Calibration mode agreement rates:\n",
      "     y_none: 0.120\n",
      "   y_global: 0.117\n",
      "  y_perdrug: 0.190\n",
      "   y_shrunk: 0.175\n",
      "\n",
      "Chosen calibration mode: y_perdrug\n"
     ]
    }
   ],
   "source": [
    "# Cell 2 (fixed): compare calibration modes and pick the best — robust to column names\n",
    "\n",
    "# Build global linear calibration on line-level means\n",
    "Xg = (BASE_PREDS\n",
    "      .assign(SANGER_MODEL_ID=lambda d: d[\"SANGER_MODEL_ID\"].astype(str),\n",
    "              drug_id=lambda d: d[\"drug_id\"].astype(str))\n",
    "      .groupby([\"SANGER_MODEL_ID\",\"drug_id\"])[\"y_pred\"].mean())\n",
    "\n",
    "# Stack LNwide and rename its two key columns whatever their names are\n",
    "bulk_pairs = LNwide.stack()\n",
    "\n",
    "bp = bulk_pairs.rename(\"bulk\").reset_index()\n",
    "# Figure out the current names\n",
    "line_col = LNwide.index.name or \"SANGER_MODEL_ID\"\n",
    "drug_col = LNwide.columns.name or \"DRUG_ID\"\n",
    "# Rename to our canonical names\n",
    "bp = bp.rename(columns={line_col:\"SANGER_MODEL_ID\", drug_col:\"drug_id\"})\n",
    "bp[\"SANGER_MODEL_ID\"] = bp[\"SANGER_MODEL_ID\"].astype(str)\n",
    "bp[\"drug_id\"] = bp[\"drug_id\"].astype(str)\n",
    "\n",
    "merged_g = (\n",
    "    Xg.rename(\"sc_mean\").reset_index()\n",
    "      .merge(bp, on=[\"SANGER_MODEL_ID\",\"drug_id\"], how=\"inner\")\n",
    ")\n",
    "\n",
    "# Global fit bulk ≈ a_g * sc_mean + b_g\n",
    "if len(merged_g) >= 2 and np.var(merged_g[\"sc_mean\"].values, ddof=1) > 0:\n",
    "    a_g = np.cov(merged_g[\"sc_mean\"], merged_g[\"bulk\"], ddof=1)[0,1] / np.var(merged_g[\"sc_mean\"], ddof=1)\n",
    "else:\n",
    "    a_g = 1.0\n",
    "b_g = merged_g[\"bulk\"].mean() - a_g * merged_g[\"sc_mean\"].mean()\n",
    "\n",
    "# Per-drug calibration (fit only if >=3 lines with variance)\n",
    "rows = []\n",
    "for d, sub in merged_g.groupby(\"drug_id\"):\n",
    "    x = sub[\"sc_mean\"].values\n",
    "    y = sub[\"bulk\"].values\n",
    "    if len(sub) >= 3 and np.var(x, ddof=1) > 0:\n",
    "        a = np.cov(x, y, ddof=1)[0,1] / np.var(x, ddof=1)\n",
    "        b = y.mean() - a * x.mean()\n",
    "        rows.append({\"drug_id\": str(d), \"n_lines\": int(len(sub)), \"slope\": float(a), \"intercept\": float(b)})\n",
    "calib_df = pd.DataFrame(rows).set_index(\"drug_id\") if rows else pd.DataFrame(columns=[\"slope\",\"intercept\",\"n_lines\"])\n",
    "\n",
    "# Generate calibrated columns\n",
    "preds_cal = BASE_PREDS.copy()\n",
    "preds_cal[\"SANGER_MODEL_ID\"] = preds_cal[\"SANGER_MODEL_ID\"].astype(str)\n",
    "preds_cal[\"drug_id\"] = preds_cal[\"drug_id\"].astype(str)\n",
    "preds_cal[\"y_none\"]   = preds_cal[\"y_pred\"].astype(float)\n",
    "preds_cal[\"y_global\"] = a_g * preds_cal[\"y_pred\"].astype(float) + b_g\n",
    "\n",
    "preds_cal[\"y_perdrug\"] = preds_cal[\"y_pred\"].astype(float)\n",
    "preds_cal[\"y_shrunk\"]  = preds_cal[\"y_pred\"].astype(float)\n",
    "LAMBDA = 5.0  # shrinkage toward global\n",
    "\n",
    "if len(calib_df):\n",
    "    for d, row in calib_df.iterrows():\n",
    "        m = preds_cal[\"drug_id\"].eq(d)\n",
    "        a_d, b_d, n = float(row[\"slope\"]), float(row[\"intercept\"]), int(row[\"n_lines\"])\n",
    "        preds_cal.loc[m, \"y_perdrug\"] = a_d * preds_cal.loc[m, \"y_pred\"].astype(float) + b_d\n",
    "        w = n / (n + LAMBDA)\n",
    "        a_s = w*a_d + (1-w)*a_g\n",
    "        b_s = w*b_d + (1-w)*b_g\n",
    "        preds_cal.loc[m, \"y_shrunk\"] = a_s * preds_cal.loc[m, \"y_pred\"].astype(float) + b_s\n",
    "\n",
    "def percentile_of_value(arr, value):\n",
    "    arr = np.asarray(arr, float)\n",
    "    if arr.size == 0 or np.isnan(value): return np.nan\n",
    "    lt = np.sum(arr < value); eq = np.sum(arr == value)\n",
    "    return 100.0 * (lt + 0.5*eq) / arr.size\n",
    "\n",
    "def agreement_rate_col(col):\n",
    "    ok = 0; total = 0\n",
    "    for (line, drug), sub in preds_cal.groupby([\"SANGER_MODEL_ID\",\"drug_id\"]):\n",
    "        if (line in LNwide.index) and (drug in LNwide.columns):\n",
    "            pct = percentile_of_value(sub[col].values, float(LNwide.loc[line, drug]))\n",
    "            if not np.isnan(pct):\n",
    "                total += 1\n",
    "                ok += (25 <= pct <= 75)\n",
    "    return (ok/total) if total else np.nan\n",
    "\n",
    "modes = [\"y_none\",\"y_global\",\"y_perdrug\",\"y_shrunk\"]\n",
    "mode_rates = {m: agreement_rate_col(m) for m in modes if m in preds_cal.columns}\n",
    "best_mode = max(mode_rates.items(), key=lambda kv: (kv[1] if kv[1]==kv[1] else -1))[0]\n",
    "\n",
    "print(\"Calibration mode agreement rates:\")\n",
    "for m,r in mode_rates.items(): print(f\"  {m:>9}: {r:.3f}\")\n",
    "print(f\"\\nChosen calibration mode: {best_mode}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "id": "87bf54a4",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Saved calibration table → out/calibration_coeffs.csv\n",
      "Saved calibrated per-cell predictions → out/per_cell_predictions_perdrug.parquet  (308270 rows)\n",
      "Saved evaluation metrics → out/eval_metrics_final.csv\n",
      "Final agreement rate: 0.18571428571428572\n",
      "Saved 12 calibrated plots → out/plots_final\n",
      "\n",
      "Next: set CALIBRATION_CSV = 'out/calibration_coeffs.csv' in your File 2 notebook and re-run predictions for future runs.\n"
     ]
    }
   ],
   "source": [
    "# Cell 3: finalize outputs based on the chosen mode\n",
    "\n",
    "OUT_DIR = \"out\"\n",
    "os.makedirs(OUT_DIR, exist_ok=True)\n",
    "PLOTS_DIR = os.path.join(OUT_DIR, \"plots_final\")\n",
    "os.makedirs(PLOTS_DIR, exist_ok=True)\n",
    "\n",
    "# 3a) write calibration table you can feed back into File 2\n",
    "calib_out = None\n",
    "if best_mode == \"y_global\":\n",
    "    calib_out = pd.DataFrame({\"drug_id\": sorted(preds_cal[\"drug_id\"].unique()),\n",
    "                              \"slope\": a_g, \"intercept\": b_g})\n",
    "elif best_mode == \"y_perdrug\":\n",
    "    calib_out = calib_df.reset_index()[[\"drug_id\",\"slope\",\"intercept\"]]\n",
    "elif best_mode == \"y_shrunk\":\n",
    "    rows = []\n",
    "    if len(calib_df):\n",
    "        for d, row in calib_df.iterrows():\n",
    "            n = int(row[\"n_lines\"]); w = n/(n+5.0)\n",
    "            a_s = w*float(row[\"slope\"]) + (1-w)*a_g\n",
    "            b_s = w*float(row[\"intercept\"]) + (1-w)*b_g\n",
    "            rows.append({\"drug_id\": str(d), \"slope\": a_s, \"intercept\": b_s, \"n_lines\": n})\n",
    "        # For drugs without per-drug fit, fall back to global\n",
    "        missing = set(preds_cal[\"drug_id\"].unique()) - set(calib_df.index)\n",
    "        for d in sorted(missing):\n",
    "            rows.append({\"drug_id\": str(d), \"slope\": a_g, \"intercept\": b_g, \"n_lines\": 0})\n",
    "        calib_out = pd.DataFrame(rows)\n",
    "    else:\n",
    "        calib_out = pd.DataFrame({\"drug_id\": sorted(preds_cal[\"drug_id\"].unique()),\n",
    "                                  \"slope\": a_g, \"intercept\": b_g, \"n_lines\": 0})\n",
    "else:\n",
    "    # no calibration: identity\n",
    "    calib_out = pd.DataFrame({\"drug_id\": sorted(preds_cal[\"drug_id\"].unique()),\n",
    "                              \"slope\": 1.0, \"intercept\": 0.0})\n",
    "\n",
    "calib_path = os.path.join(OUT_DIR, \"calibration_coeffs.csv\")\n",
    "calib_out.to_csv(calib_path, index=False)\n",
    "print(f\"Saved calibration table → {calib_path}\")\n",
    "\n",
    "# 3b) write calibrated per-cell predictions\n",
    "col_map = {\"y_none\":\"per_cell_predictions_uncal.parquet\",\n",
    "           \"y_global\":\"per_cell_predictions_global.parquet\",\n",
    "           \"y_perdrug\":\"per_cell_predictions_perdrug.parquet\",\n",
    "           \"y_shrunk\":\"per_cell_predictions_shrunk.parquet\"}\n",
    "preds_save = preds_cal[[\"cell_id\",\"SANGER_MODEL_ID\",\"drug_id\", best_mode]].rename(columns={best_mode:\"y_pred\"})\n",
    "preds_path = os.path.join(OUT_DIR, col_map.get(best_mode, \"per_cell_predictions_final.parquet\"))\n",
    "preds_save.to_parquet(preds_path, index=False)\n",
    "print(f\"Saved calibrated per-cell predictions → {preds_path}  ({len(preds_save)} rows)\")\n",
    "\n",
    "# 3c) evaluation table + quick plots\n",
    "rows = []\n",
    "for (line, drug), sub in preds_save.groupby([\"SANGER_MODEL_ID\",\"drug_id\"]):\n",
    "    if (line not in LNwide.index) or (drug not in LNwide.columns): continue\n",
    "    y = sub[\"y_pred\"].values\n",
    "    bulk_val = float(LNwide.loc[line, drug])\n",
    "    lt = percentile_of_value(y, bulk_val)\n",
    "    rows.append({\n",
    "        \"SANGER_MODEL_ID\": line,\n",
    "        \"drug_id\": drug,\n",
    "        \"n_cells\": int(len(y)),\n",
    "        \"pred_mean\": float(np.mean(y)),\n",
    "        \"pred_q10\": float(np.quantile(y, 0.10)),\n",
    "        \"pred_q90\": float(np.quantile(y, 0.90)),\n",
    "        \"bulk_LN_IC50\": bulk_val,\n",
    "        \"bulk_percentile_in_pred\": float(lt),\n",
    "        \"agreement_label\": \"agree\" if 25<=lt<=75 else (\"discordant\" if lt<10 or lt>90 else \"borderline\"),\n",
    "    })\n",
    "eval_final = pd.DataFrame(rows).sort_values([\"SANGER_MODEL_ID\",\"drug_id\"]).reset_index(drop=True)\n",
    "eval_path = os.path.join(OUT_DIR, \"eval_metrics_final.csv\")\n",
    "eval_final.to_csv(eval_path, index=False)\n",
    "print(f\"Saved evaluation metrics → {eval_path}\")\n",
    "print(\"Final agreement rate:\", (eval_final[\"agreement_label\"]==\"agree\").mean())\n",
    "\n",
    "# few plots\n",
    "def plot_dist(line, drug, df_mode):\n",
    "    sub = df_mode[(df_mode[\"SANGER_MODEL_ID\"]==line) & (df_mode[\"drug_id\"]==drug)]\n",
    "    if sub.empty: return\n",
    "    y = sub[\"y_pred\"].values\n",
    "    bulk_val = float(LNwide.loc[line, drug])\n",
    "    plt.figure(figsize=(6,4))\n",
    "    plt.hist(y, bins=40, alpha=0.7, density=True)\n",
    "    plt.axvline(bulk_val, linestyle=\"--\", linewidth=2, label=f\"Bulk LN_IC50 = {bulk_val:.2f}\")\n",
    "    plt.title(f\"{line} — drug {drug} ({best_mode})\")\n",
    "    plt.xlabel(\"Per-cell predicted LN_IC50\")\n",
    "    plt.ylabel(\"Density\")\n",
    "    plt.legend(loc=\"best\")\n",
    "    plt.tight_layout()\n",
    "    fname = os.path.join(PLOTS_DIR, f\"dist_{best_mode}_{line}_{drug}.png\")\n",
    "    plt.savefig(fname, dpi=150); plt.close()\n",
    "\n",
    "made = 0\n",
    "for (line, drug), _ in preds_save.groupby([\"SANGER_MODEL_ID\",\"drug_id\"]):\n",
    "    plot_dist(line, drug, preds_save)\n",
    "    made += 1\n",
    "    if made >= 12: break\n",
    "print(f\"Saved {made} calibrated plots → {PLOTS_DIR}\")\n",
    "\n",
    "print(\"\\nNext: set CALIBRATION_CSV = 'out/calibration_coeffs.csv' in your File 2 notebook and re-run predictions for future runs.\")\n"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "tese-env",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.16"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
